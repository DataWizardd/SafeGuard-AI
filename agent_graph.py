import os
import re
from typing import TypedDict, List
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from rag_setup import get_retriever
from pdf_gen import generate_permit_pdf

# LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o", temperature=0)
retriever = get_retriever()

# --- [NEW] í”„ë¡¬í”„íŠ¸ ë¡œë” í•¨ìˆ˜ ---
def load_prompt(filename, **kwargs):
    """
    prompts í´ë”ì˜ md íŒŒì¼ì„ ì½ì–´ì„œ ë³€ìˆ˜({key})ë¥¼ ì±„ì›Œì£¼ëŠ” í•¨ìˆ˜
    """
    file_path = os.path.join("prompts", filename)
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            # íŒŒì¼ ë‚´ìš©ì— ë³€ìˆ˜ê°’ ì£¼ì… (format ì‚¬ìš©)
            return content.format(**kwargs)
    except Exception as e:
        print(f"âŒ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
        return ""

# --- 1. ìƒíƒœ(State) ì •ì˜ ---
class AgentState(TypedDict):
    user_input: str
    chat_history: str
    messages: List[str]
    context: str
    risk_level: str
    risk_score: int
    final_output: str
    pdf_path: str
    needs_more_info: bool

# --- 2. ë…¸ë“œ(Agent) ì •ì˜ ---

def coordinator(state: AgentState):
    """Main Orchestrator: ì˜ë„ íŒŒì•… ë° ì •ë³´ ë³‘í•©"""
    print("ğŸ¤– [Coordinator] ì§€ëŠ¥í˜• ë¶„ì„ ì¤‘...")
    
    # [ìˆ˜ì •] íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = load_prompt(
        "coordinator.md", 
        chat_history=state.get('chat_history', 'ì—†ìŒ'),
        user_input=state['user_input']
    )
    
    response = llm.invoke([HumanMessage(content=prompt)]).content
    
    if response.startswith("MISSING"):
        question = response.replace("MISSING:", "").strip()
        return {"needs_more_info": True, "messages": [question]}
    
    return {"needs_more_info": False}

def regulation_finder(state: AgentState):
    print("ğŸ“š [Regulation Agent] ê·œì • ê²€ìƒ‰ ì¤‘ (ì´ì¤‘ ê²€ìƒ‰)...")
    query = state['user_input']
    
    # [ì „ëµ 1] ì¼ë°˜ ê²€ìƒ‰ (ë²•ë ¹, ê°€ì´ë“œë¶ ìœ„ì£¼)
    docs_general = retriever.invoke(query)
    
    # [ì „ëµ 2] ì‚¬ë‚´ ê·œì • ê°•ì œ ê²€ìƒ‰ (S-Chem í‚¤ì›Œë“œ ì¶”ê°€)
    # ì¿¼ë¦¬ì— íšŒì‚¬ ì´ë¦„ì„ ê°•ì œë¡œ ë¶™ì—¬ì„œ ê²€ìƒ‰ê¸°ê°€ ì‚¬ë‚´ ê·œì •ì„ ì°¾ë„ë¡ ìœ ë„
    company_query = f"{query} S-Chem ì‚¬ë‚´ ì•ˆì „ ì‘ì—… ê·œì • ì ˆì°¨"
    docs_company = retriever.invoke(company_query)
    
    # [ì „ëµ 3] ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì¡°ì •)
    # ì‚¬ë‚´ ê·œì • ê²€ìƒ‰ ê²°ê³¼(docs_company)ë¥¼ ë¦¬ìŠ¤íŠ¸ ì•ìª½ì— ë°°ì¹˜í•˜ì—¬ ê°•ì¡°
    combined_docs = docs_company[:2] + docs_general 
    
    # ì¤‘ë³µ ì œê±° (ë‚´ìš© ê¸°ì¤€)
    seen = set()
    unique_docs = []
    for doc in combined_docs:
        # ë¬¸ì„œ ë‚´ìš©ì˜ ì• 50ê¸€ìë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì²´í¬
        doc_key = doc.page_content[:50]
        if doc_key not in seen:
            seen.add(doc_key)
            unique_docs.append(doc)
    
    # ìµœì¢… ìƒìœ„ 6~8ê°œë§Œ ì„ íƒ
    final_docs = unique_docs[:8]

    # [ë””ë²„ê¹…] ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ” ìµœì¢… í™•ë³´ëœ ë¬¸ì„œ: {len(final_docs)}ê±´")

    if not final_docs:
        return {"context": "ê´€ë ¨ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    formatted_docs = []
    for doc in final_docs:
        filename = os.path.basename(doc.metadata.get("source", "íŒŒì¼_ì—†ìŒ"))
        content = doc.page_content.strip()
        if not content: continue
            
        formatted_docs.append(f"ğŸ“„ [ì¶œì²˜: {filename}]\n{content}")
    
    context_text = "\n\n---\n\n".join(formatted_docs)
    return {"context": context_text}

def risk_analyst(state: AgentState):
    """Fine-Kinney ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì •ëŸ‰ì  ìœ„í—˜ì„± í‰ê°€"""
    print("âš ï¸ [Risk Analyst] ìœ„í—˜ë„ ê³„ì‚° ì¤‘ (Fine-Kinney)...")
    
    # [ìˆ˜ì •] íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = load_prompt(
        "risk_analyst.md",
        chat_history=state.get('chat_history', 'ì—†ìŒ'),
        user_input=state['user_input'],
        context=state['context']
    )
    
    response = llm.invoke([HumanMessage(content=prompt)]).content
    
    try:
        # ì •ê·œí‘œí˜„ì‹ íŒŒì‹±
        p_match = re.search(r"P\s*[:=]\s*([\d\.]+)", response)
        e_match = re.search(r"E\s*[:=]\s*([\d\.]+)", response)
        c_match = re.search(r"C\s*[:=]\s*([\d\.]+)", response)
        r_match = re.search(r"R\s*[:=]\s*([\d\.]+)", response)
        
        p_score = float(p_match.group(1)) if p_match else 0
        e_score = float(e_match.group(1)) if e_match else 0
        c_score = float(c_match.group(1)) if c_match else 0
        
        if r_match:
            r_score = float(r_match.group(1))
        else:
            r_score = p_score * e_score * c_score
            
        type_match = re.search(r"ì¬í•´ìœ í˜•\s*[:=]\s*(.+)", response)
        accident_type = type_match.group(1).strip() if type_match else "ë³µí•© ìœ„í—˜"

        if r_score >= 320: level = "Very High"
        elif r_score >= 160: level = "High"
        elif r_score >= 70: level = "Medium"
        else: level = "Low"
        
        final_report = f"""
**ğŸ¯ Fine-Kinney ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼**
* **ì¬í•´ í˜•íƒœ:** {accident_type}
* **ê³„ì‚° ê³µì‹:** $Risk = P \\times E \\times C$
* **ìƒì„¸ ì ìˆ˜:**
    * ê°€ëŠ¥ì„±(P): **{p_score}**
    * ë…¸ì¶œë¹ˆë„(E): **{e_score}**
    * ê°•ë„(C): **{c_score}**
* **ìµœì¢… ìœ„í—˜ë„(R):** <span style='color:red; font-size:1.2em; font-weight:bold;'>{int(r_score)}ì </span> ({level})
"""
    except Exception as e:
        print(f"íŒŒì‹± ì—ëŸ¬: {e} / LLM ì‘ë‹µ: {response}")
        r_score = 0; level = "Error"; final_report = "ìœ„í—˜ì„± í‰ê°€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return {"risk_score": int(r_score), "risk_level": level, "context": state['context'] + "\n\n" + final_report}

def admin_agent(state: AgentState):
    """ìµœì¢… PDF ìƒì„± ë° ë©”ì‹œì§€ ì‘ì„± (í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¶„ë¦¬ ë²„ì „)"""
    print("ğŸ“ [Admin Agent] ì‘ì—… ë‚´ìš© ìš”ì•½ ë° PDF ìƒì„± ì¤‘...")
    
    score = state['risk_score']
    context = state['context']
    history = state.get('chat_history', '')
    last_input = state['user_input']
    
    # ------------------------------------------------------------------
    # [STEP 1] ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ 'í†µí•© ì‘ì—… ë‚´ìš©' ìš”ì•½í•˜ê¸°
    # ------------------------------------------------------------------
    # [ìˆ˜ì •] í•˜ë“œì½”ë”© ëŒ€ì‹  work_summary.md íŒŒì¼ ë¡œë“œ
    summary_prompt = load_prompt(
        "work_summary.md",
        history=history,
        last_input=last_input
    )
    
    # ë§Œì•½ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ë¹„ìš© ì•ˆì „ì¥ì¹˜
    if not summary_prompt:
        summary_prompt = f"ëŒ€í™”ê¸°ë¡: {history}\në§ˆì§€ë§‰ì…ë ¥: {last_input}\nìœ„ ë‚´ìš©ì„ í¬í•¨í•´ ì‘ì—… ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´."

    # ì‘ì—… ì œëª©ì„ LLMì´ ë‹¤ì‹œ ì”ë‹ˆë‹¤.
    consolidated_work_info = llm.invoke([HumanMessage(content=summary_prompt)]).content.replace('"', '').strip()
    print(f"ğŸ“Œ í†µí•©ëœ ì‘ì—… ë‚´ìš©: {consolidated_work_info}")

    # ------------------------------------------------------------------
    # [STEP 2] ìœ„í—˜ ìš”ì¸ ë¶„ì„
    # ------------------------------------------------------------------
    # admin_agent.md íŒŒì¼ ë¡œë“œ
    reasoning_prompt_content = load_prompt(
        "admin_agent.md",
        user_input=consolidated_work_info, # ìš”ì•½ëœ ë‚´ìš©ì„ ë„£ì–´ì¤Œ
        context=context
    )
    
    reason_summary = llm.invoke([HumanMessage(content=reasoning_prompt_content)]).content
    
    # ------------------------------------------------------------------
    # [STEP 3] PDF ìƒì„±
    # ------------------------------------------------------------------
    try:
        # ìš”ì•½ëœ ì‘ì—… ë‚´ìš©(consolidated_work_info)ì„ PDF ì œëª©ìœ¼ë¡œ ì „ë‹¬
        pdf_file = generate_permit_pdf(score, state['risk_level'], reason_summary, consolidated_work_info)
    except Exception as e:
        print(f"PDF ì—ëŸ¬: {e}")
        pdf_file = None
    
    # UI ë©”ì‹œì§€ ìƒì„±
    if score >= 160:
        short_msg = f"ğŸš¨ **ë°˜ë ¤ (High Risk / {score}ì )**\nìƒì„¸ ì‚¬ìœ ëŠ” PDF í™•ì¸ í•„ìš”."
    elif score >= 70:
        short_msg = f"âš ï¸ **ì¡°ê±´ë¶€ ìŠ¹ì¸ (Medium Risk / {score}ì )**\nì•ˆì „ ì¡°ì¹˜ ì´í–‰ í›„ ì‘ì—… ê°€ëŠ¥."
    else:
        short_msg = f"âœ… **ìŠ¹ì¸ (Low Risk / {score}ì )**\nì‘ì—… í—ˆê°€ì„œ ë°œê¸‰ ì™„ë£Œ."
        
    return {"final_output": short_msg, "pdf_path": pdf_file}

# --- 3. ê·¸ë˜í”„ ì—°ê²° ---
workflow = StateGraph(AgentState)
workflow.add_node("coordinator", coordinator)
workflow.add_node("regulation_finder", regulation_finder)
workflow.add_node("risk_analyst", risk_analyst)
workflow.add_node("admin_agent", admin_agent)
workflow.set_entry_point("coordinator")

def check_info(state):
    return "end" if state['needs_more_info'] else "next"

workflow.add_conditional_edges("coordinator", check_info, {"end": END, "next": "regulation_finder"})
workflow.add_edge("regulation_finder", "risk_analyst")
workflow.add_edge("risk_analyst", "admin_agent")
workflow.add_edge("admin_agent", END)

app_graph = workflow.compile()